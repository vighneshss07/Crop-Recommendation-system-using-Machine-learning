# -*- coding: utf-8 -*-
"""New Crop recommendation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ed9KgZ8Zq6zCl_fGXTkd5_GWX2Jht1Me
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from scipy.interpolate import make_interp_spline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score as acc
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import plotly.graph_objects as go
from sklearn.utils import shuffle
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import r2_score
from sklearn.datasets import make_classification
from sklearn.multioutput import MultiOutputClassifier
from sklearn.ensemble import RandomForestClassifier

"""**Data Loading**"""

df = pd.read_csv('/content/drive/MyDrive/Crop Recommendation/C_R.csv')
print('Data Shape: ', df.shape)

df.head()

"""# **Data fields**

N - ratio of Nitrogen content in soil

P - ratio of Phosphorous content in soil

K - ratio of Potassium content in soil

temperature - temperature in degree Celsius

humidity - relative humidity in %

ph - ph value of the soil

rainfall - rainfall in mm

#**EDA**
"""

## info

df.info()

# type of data

df.dtypes

df.isna().sum()

# Unique Name of the Crops in Dataset
crops = df['label'].unique()
crops.sort()
print ("Total Number of Crops Data: ", len(crops) )
print("\n","-"*20, " List of Crops ", "-"*20)
crops.tolist()

df.label.unique()

# get top 5 most frequent growing crops
n = 5
df['label'].value_counts()[:5].index.tolist()

# Number of Rows against each Crop
print("Number of Records Against Eash Crop")
print("-"*35)
print(df['label'].value_counts())

#Columns Name
df.columns

# Features Selection
selected_features = {'N', 'P', 'K', 'Temperature', 'Humidity', 'ph', 'Rainfall'}
selected_features

## descripe

df.describe()

"""**Observation:** Since Mean and Median are almost similar, no scaling is required."""

all_columns = df.columns[:-1]

plt.figure(figsize=(15,13))
i = 1
for column in all_columns[:-1]:
    plt.subplot(3,3,i)
    sns.histplot(df[column]) #A "subplot" refers to a smaller plot or chart that exists within a larger plot, allowing you to display multiple plots in a single figure.
    i+=1
plt.show()

sns.histplot(df[all_columns[-1]])
plt.show()

p = sns.pairplot(df) #different for each and same column type

for column in all_columns:
    plt.figure(figsize=(19,7))
    sns.barplot(x = "label", y = column, data = df)
    plt.xticks(rotation=90)
    plt.title(f"{column} vs Crop Type")
    plt.show()

# Convert non-numeric columns to numeric if possible
df_numeric = df.apply(pd.to_numeric, errors='coerce')
correlation_matrix = df_numeric.corr()

import seaborn as sns
import matplotlib.pyplot as plt

# Convert non-numeric columns to numeric if possible
df_numeric = df.apply(pd.to_numeric, errors='coerce')

# Create the correlation matrix
correlation_matrix = df_numeric.corr()

# Plot the heatmap
#A heatmap is a graphical representation of data where the values are depicted using a color scale.
sns.heatmap(correlation_matrix, annot=True)
plt.title('Correlation Matrix')
plt.show()

# Shuffling data to remove order effects

# shuffling the dataset to remove order

df  = shuffle(df,random_state=5)
df.head()
# df randomly while maintaining the relationship between the features and the corresponding labels. It uses a random seed (random_state=5) to ensure reproducibility,
#meaning that if you run the code with the same seed, you'll get the same shuffled DataFrame.
#After shuffling, df.head() displays the first few rows of the shuffled DataFrame

"""#**Assumptions for Linear Regression**

##**1. Linearity**
"""

p = sns.pairplot(df, x_vars=['Temperature',	'Humidity',	'ph',	'Rainfall'], y_vars='label', size=7, aspect=0.7)

#pairplot function. It focuses on the relationship between the specified independent variables ('Temperature', 'Humidity', 'ph', 'Rainfall') and the dependent variable ('label').

# visualize the relationship between the features and the response using scatterplots
p = sns.pairplot(df, x_vars=['N',	'P',	'K'], y_vars='label', size=7, aspect=0.7)

"""#**Encoding target variable**"""

xdf = df.copy()
xdf
#creates a deep copy of the DataFrame

labelencoder= LabelEncoder() # initializing an object of class LabelEncoder

#Fit and Transforming the label column.
xdf['label_codes'] = labelencoder.fit_transform(xdf['label'])
xdf

y = xdf['label_codes']  # Targeted Values Selection
X = xdf[list(selected_features)]  # Independent Values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

#test_size=0.3 means that 30% of the data will be used for testing, and the remaining 70% will be used for training.

X

y

"""**Splitting the data into training data and testing data.**"""

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)
print(X.shape,X_train.shape,X_test.shape)

"""**Model build- linear .fit .predict**"""

from sklearn.linear_model import LinearRegression
LR=LinearRegression()  ## importing the model object
LR.fit(X_train,y_train) ## fitting the training data

X_test_pred=LR.predict(X_test) #x test feature matrix

X_test_pred

y_test

#label_code

LR1=LinearRegression()
LR1.fit(X_train,y_train)
y_pred = LR1.predict(X_train) #x train feature matrix
#calling the fit method involves adjusting the parameters of the model such that it best fits the training data.

y_pred

#label_code

LR.fit(X_test, y_test)
y_pred_1 = LR.predict(X_test)
y_pred_1

#calling the fit method involves adjusting the parameters of the model such that it best fits the training data.

X_train_pred=LR.predict(X_train)

X_train_pred

y_train
#label_code

"""**Prediction**"""

from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error
## prediction on training data

train_score=r2_score(y_train,X_train_pred)

print('Linear Regression training score :',train_score)

## prediction on testing data

test_score=r2_score(y_test,X_test_pred)

print('Linear Regression testing score :',test_score)

"""**Mean squared error and mean absolute error.**"""

## mean squared error and mean absolute error of training data

print('MSE:',mean_squared_error(y_train,X_train_pred))
print('MAE:',mean_absolute_error(y_train,X_train_pred))

## mean squared error and mean absolute error of testing data

print('MSE',mean_squared_error(y_test,X_test_pred))
print('MAE',mean_absolute_error(y_test,X_test_pred))

"""**Root mean squared error**"""

import math
print(math.sqrt(mean_squared_error(y_test,X_test_pred)))
#calculates the square root of the mean squared error (RMSE) between the true labels (y_test) and the predicted labels (X_test_pred).

print(math.sqrt(mean_squared_error(y_train,X_train_pred)))

X_train.shape,X_test.shape

"""**Adjusted r2 score**"""

train_adjusted_r2score=1-(1-0.71)*(1003-1)/(1003-6-1)
train_adjusted_r2score

#train

test_adjusted_r2score=1-(1-0.74)*(335-1)/(335-6-1)
test_adjusted_r2score

#test

"""**2. Mean of Residuals**"""

residuals = y_train.values-y_pred
mean_residuals = np.mean(residuals)
print("Mean of Residuals {}".format(mean_residuals))

#the residuals by subtracting the predicted values (y_pred) from the actual values (y_train.values).
#train

residuals_1 = y_test.values-y_pred_1
mean_residuals_1 = np.mean(residuals_1)
print("Mean of Residuals {}".format(mean_residuals_1))

#test

"""**3. Check for Homoscedasticity**

Detecting heteroscedasticity!
"""

import matplotlib.pyplot as plt

plt.scatter(y_pred, residuals)
plt.xlabel('y_pred/predicted values')
plt.ylabel('Residuals')
plt.ylim(-15, 15)
plt.xlim(0, 50)
plt.plot([0, 26], [0, 0], color='blue')  # Adding a line from (0,0) to (26,0)
plt.title('Residuals vs fitted values plot for homoscedasticity check')
plt.show()

#train
#the differences between the observed and predicted values) is constant across all levels of the independent variables.

import matplotlib.pyplot as plt

plt.scatter(y_pred_1, residuals_1)
plt.xlabel('y_pred_test/predicted values')
plt.ylabel('Residuals')
plt.ylim(-15, 15)
plt.xlim(0, 26)
plt.plot([0, 26], [0, 0], color='blue')  # Adding a line from (0,0) to (26,0)
plt.title('Residuals vs fitted values plot for homoscedasticity check')
plt.show()

#test

"""**Goldfeld Quandt Test**"""

import statsmodels.stats.api as sms
from statsmodels.compat import lzip
name = ['F statistic', 'p-value']
test = sms.het_goldfeldquandt(residuals, X_train)
lzip(name, test)


#the Goldfeld-Quandt test to check for heteroscedasticity in the residuals of a regression model

#The Goldfeld-Quandt test returns two test statistics: the F statistic and the p-value.
#The F statistic measures the difference in variance between two subsets of the data, while the p-value indicates the significance of the test.
#If the p-value is less than a chosen significance level (e.g., 0.05), it suggests evidence against the null hypothesis of homoscedasticity, indicating heteroscedasticity in the residuals.

name = ['F statistic', 'p-value']
test = sms.het_goldfeldquandt(residuals_1, X_test)
lzip(name, test)

"""**4. Check for Normality of error terms/residuals**"""

plt.figure(figsize=(5,8))
p = sns.distplot(residuals,kde=True)
p = plt.title('Normality of error terms/residuals')

plt.figure(figsize=(5,8))
p = sns.distplot(residuals_1,kde=True)
p = plt.title('Normality of error terms/residuals')

import statsmodels.api as sm
# autocorrelation
# The ACF measures the correlation between the residuals at different lags (time intervals).
plt.figure(figsize=(2,5)) #width and height of figure
sm.graphics.tsa.plot_acf(residuals, lags=40)
plt.show()

sm.graphics.tsa.plot_acf(residuals_1, lags=40)
plt.show()

# partial autocorrelation
sm.graphics.tsa.plot_pacf(residuals, lags=40)
plt.show()

# partial autocorrelation
sm.graphics.tsa.plot_pacf(residuals_1, lags=40)
plt.show()

"""**Shuffling data to remove order effects**"""

# shuffling the dataset to remove order
from sklearn.utils import shuffle

df  = shuffle(df,random_state=5)
df.head()

#while maintaining the relationship between the features and the corresponding labels

# Selection of Feature and Target variables.

x = df[['N', 'P','K','Temperature', 'Humidity', 'ph', 'Rainfall']]
target = df['label']

# Encoding target variable

y = pd.get_dummies(target)
y
#pd.get_dummies(target) is used to convert categorical variables into dummy/indicator variables

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25, random_state= 0)

print("x_train :",x_train.shape)
print("x_test :",x_test.shape)
print("y_train :",y_train.shape)
print("y_test :",y_test.shape)

# Training

forest = RandomForestClassifier(random_state=1)
multi_target_forest = MultiOutputClassifier(forest, n_jobs=-1)
multi_target_forest.fit(x_train, y_train)

# Predicting test results

forest_pred = multi_target_forest.predict(x_test)
forest_pred

# Calculating Accuracy

from sklearn.metrics import accuracy_score
a1 = accuracy_score(y_test, forest_pred)
print('Accuracy score:', accuracy_score(y_test, forest_pred))

"""**Cross-validation**"""

from sklearn.model_selection import cross_val_score
score = cross_val_score(multi_target_forest,X = x_train, y = y_train,cv=5)
score

b1 = "{:.2f}".format(score.mean()*100)
b1 = float(b1)
b1

c1 = (score.std()*100)
c1

print("Accuracy : {:.2f}%".format (score.mean()*100))
print("Standard Deviation : {:.2f}%".format(score.std()*100))

# Training
from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=6)
multi_target_decision = MultiOutputClassifier(clf, n_jobs=-1)
multi_target_decision.fit(x_train, y_train)

# Predicting test results

decision_pred = multi_target_decision.predict(x_test)
decision_pred

# Calculating Accuracy

from sklearn.metrics import accuracy_score
a2 = accuracy_score(y_test,decision_pred)
print('Accuracy score:', accuracy_score(y_test,decision_pred))
a2

"""**Cross-validation**"""

score = cross_val_score(multi_target_decision,X = x_train, y = y_train,cv=7)
score

b2 = "{:.2f}".format(score.mean()*100)
b2 = float(b2)
b2

c2 = (score.std()*100)
c2

from sklearn.neighbors import KNeighborsClassifier

knn_clf=KNeighborsClassifier()
model = MultiOutputClassifier(knn_clf, n_jobs=-1)
model.fit(x_train, y_train)

knn_pred = model.predict(x_test)
knn_pred

# Calculating Accuracy

knn_pred = model.predict(x_test)
knn_pred

"""**Cross validation**"""

score = cross_val_score(model,X = x_train, y = y_train,cv=7)
score

b3 = "{:.2f}".format(score.mean()*100)
b3 = float(b3)
b3

c3 = (score.std()*100)
c3

"""#**Creates pandas DataFrame.**"""

# initialise data of lists.
data = {'Algorithms':['Random Forest', 'Decision-tree', 'KNN Classifier'],
        'Accuracy':[b1, b2, b3],
        'Standard Deviation':[c1,c2,c3]}


df = pd.DataFrame(data)

# print the data
df

# create a dataset
Algorithms = ['Random Forest', 'Decision-tree','KNN Classifier']
Accuracy = [b1, b2, b3]

x_pos = np.arange(len(Accuracy))

# Create bars with different colors
plt.bar(x_pos, Accuracy, color=['#488AC7','#ff8c00','#009150'])

# Create names on the x-axis
plt.xticks(x_pos, Algorithms)
plt.ylabel('Accuracy(in %)')
plt.xlabel('Machine Learning Classifying Techniques')

# Show graph
plt.show()

# create a dataset
Algorithms = ['Random Forest', 'Decision-tree','KNN Classifier']
Accuracy = [b1, b2, b3]

x_pos = np.arange(len(Accuracy))

# Create bars with different colors
plt.bar(x_pos, Accuracy, color=['#488AC7','#ff8c00','#009150'])

# Create names on the x-axis
plt.xticks(x_pos, Algorithms)
plt.ylabel('Accuracy(in %)')
plt.xlabel('Machine Learning Classifying Techniques')

# Show graph
plt.show()

# create a dataset
Algorithms = ['Random Forest', 'Decision-tree','KNN']
Accuracy = [c1, c2, c3]

x_pos = np.arange(len(Accuracy))

# Create bars with different colors
plt.bar(x_pos, Accuracy, color= ['#488AC7','#ff8c00','#009150'])

# Create names on the x-axis
plt.xticks(x_pos, Algorithms)
plt.ylabel('Standard Deviation(in %)')
plt.xlabel('Machine Learning Classifying Techniques')

# Show graph
plt.show()

def addlabels(x,y):
    for i in range(len(x)):
        plt.text(i,y[i],y[i],ha = 'center')

if __name__ == '__main__':
    # creating data on which bar chart will be plot
    x = ["Random Forest", "Decision tree", "KNN"]
    y = [b1,b2,b3]


    x_pos = np.arange(len(y))

    # Create bars with different colors
    plt.bar(x_pos, y, color= ['#A52A2A','#00008B','#2E8B57'])

    # calling the function to add value labels
    addlabels(x, y)

    # giving X and Y labels
    plt.xlabel("Machine Learning Classifying Algorithms")
    plt.ylabel("Accuracy (in %)")

    # visualizing the plot
    plt.show()

"""#bff1f1
#fadadd
#ddfada
sourajita dewasi9:47 PM
#e7dafa
#bb97f1"""

plt.bar(df['Algorithms'], df['Accuracy'], color = ['#A52A2A','#00008B','#2E8B57'])

plt.title('Crop Suggestion Model')

# Show Plot
plt.show()

# set width of bar
barWidth = 0.25
fig = plt.subplots(figsize =(10, 6))

# set height of bar
Algorithms = ['Random Forest', 'Decision-tree', 'KNN Classifier']
Accuracy = [b1, b2, b3]
Standard_Deviation = [c1,c2,c3]

# Set position of bar on X axis
br1 = np.arange(len(Accuracy))
br2 = [x + barWidth for x in br1]
br3 = [x + barWidth for x in br2]

# Make the plot
plt.bar(br1, Accuracy, color ='blue', width = barWidth,
        edgecolor ='grey', label ='Accuracy')
plt.bar(br2, Standard_Deviation, color ='maroon', width = barWidth,
        edgecolor ='grey', label ='Standard Devation')

# Adding Xticks
plt.xlabel('Algorithms', fontweight ='bold', fontsize = 10)
plt.ylabel('Accuracy (in %)', fontweight ='bold', fontsize = 10)
plt.xticks([r + barWidth for r in range(len(Accuracy))],
        Algorithms)

plt.legend()
plt.show()

"""#**Saving the trained Random Forest model.**"""

import pickle
# Dump the trained Naive Bayes classifier with Pickle
RF_pkl_filename = 'RandomForest.pkl'
# Open the file to save as pkl file
RF_Model_pkl = open(RF_pkl_filename, 'wb')
pickle.dump(multi_target_forest, RF_Model_pkl)
# Close the pickle instances
RF_Model_pkl.close()